{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8e815d-dd37-4b31-ae56-dfc2f06821c4",
   "metadata": {},
   "source": [
    "## Text Analysis and Natural Language Processing Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e6acf-9e26-4c9a-9c1d-00b40367bad2",
   "metadata": {},
   "source": [
    "##### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c4cabce-01b6-424c-86f3-1f87feb25336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"This is a sample sentence.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea75a44-4000-4e1b-ac01-cbbfdd50cb35",
   "metadata": {},
   "source": [
    "##### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b33a7c8f-d34e-4e25-a1ab-cd2bf0c62d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"running\"))  # Output: run\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"running\", pos='v'))  # Output: run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf69932-c75d-469b-93ca-ab443f081690",
   "metadata": {},
   "source": [
    "##### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "576585e9-7910-45fe-b10e-c8cf63953840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Tokenize and tag the sentence\n",
    "tokens = word_tokenize(\"This is a sample sentence.\")\n",
    "tagged = pos_tag(tokens)\n",
    "print(tagged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ef03d-8c84-4f01-b080-abf511426b5f",
   "metadata": {},
   "source": [
    "## Corpora for Various Language Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf9ccfa-1317-4e2c-98e6-a34a846c38fd",
   "metadata": {},
   "source": [
    "##### Gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7c9a5b2-0500-4901-96f4-071737b08c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f2506-e716-4b8f-93e2-5cd67669a94b",
   "metadata": {},
   "source": [
    "##### brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1b247775-d5c2-4dc9-a558-5d7114f42de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "print(brown.categories())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3943a32e-de8b-4fec-b9e5-4c57a59528b6",
   "metadata": {},
   "source": [
    "###### WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cd0f81d6-2e6d-4d0e-b3e1-5e57f4a6ef85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"program\")\n",
    "print(syns[0].lemmas()[0].name())  # Output: program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166c138-a7a7-4e6e-8c59-7108bba1973c",
   "metadata": {},
   "source": [
    "## Training and Testing NLP Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb32daf-73c2-469f-969d-8890d924d013",
   "metadata": {},
   "source": [
    "##### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "12513e5c-f2b2-42b0-97b1-6ec79db25ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29fb4d0-cabf-4381-96fe-c90c98fe6e51",
   "metadata": {},
   "source": [
    "##### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d9475a0a-22a7-42e5-b5e3-99d41d9733cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002\n",
    "train_sents = conll2002.iob_sents('esp.train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0249ee-cc78-4105-9d2f-21ad4cd65c86",
   "metadata": {},
   "source": [
    "## Lexical and Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb17ad7-0769-4fc4-97ca-84baeca86700",
   "metadata": {},
   "source": [
    "##### Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9bb6356f-c7fb-40b2-b5a0-3d2d78a6b564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'is'), ('a', 'sample'), ('is', 'a'), ('sample', 'sentence'), ('sentence', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "bigrams = bigram_finder.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "print(bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ff37d-1065-4170-ae43-ea34826c41e0",
   "metadata": {},
   "source": [
    "##### Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "154b8a02-4888-4515-82e9-f77e3a374122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      "This is a sample sentence . This is another example sentence\n",
      "sentence . This is another example sentence .\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import Text\n",
    "text = Text(word_tokenize(\"This is a sample sentence. This is another example sentence.\"))\n",
    "text.concordance(\"sentence\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a1013-b031-4a2d-96ca-35277376cff6",
   "metadata": {},
   "source": [
    "## Language Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2fc68-d5fd-4d0b-935f-470a0ce66413",
   "metadata": {},
   "source": [
    "##### N-gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1d4615f9-81bb-4e5c-a9ea-a1a606582457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', 'is')\n",
      "('is', 'a')\n",
      "('a', 'sample')\n",
      "('sample', 'sentence')\n",
      "('sentence', '.')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "text = \"This is a sample sentence.\"\n",
    "n_grams = ngrams(word_tokenize(text), 2)\n",
    "for grams in n_grams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c9d6de-a0bd-4b3a-b6dd-181c65e76c05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9e2cb-b934-4131-9dab-f6be720c654b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
